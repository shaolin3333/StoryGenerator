from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np
from tensorflow.keras.models import load_model
from utils import beam_search, greedy_search, top_k_sampling, top_p_sampling, calculate_perplexity, calculate_bleu_score
import pickle

MAX_SEQUENCE_LEN = 256
MAX_WORDS = 50

# Load the tokenizer
tokenizer = Tokenizer()
# tokenizer_json = open('./StoryGenerator/lstm/tokenizer.pickle', 'r').read()
# tokenizer.set_config(tokenizer_json)

with open('./StoryGenerator/lstm/tokenizer.pickle', 'rb') as file:
    tokenizer = pickle.load(file)

# Load the saved LSTM model
model = load_model('./StoryGeneratorlstm/lstm_bedtime_story_model.h5')

def generate_story(seed_text, next_words, max_sequence_len):
    for _ in range(next_words):
        token_list = tokenizer.texts_to_sequences([seed_text])[0]
        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')
        predicted_probs = model.predict(token_list, verbose=0)
        predicted_word_index = np.argmax(predicted_probs, axis=1)[0]
        output_word = tokenizer.index_word.get(predicted_word_index, "")
        seed_text += " " + output_word
    return seed_text

# Generate a bedtime story
# seed_text = "Once upon a time in a magical forest"
seed_text = "prince little girl love friend happy"
# story = generate_story(seed_text, next_words=MAX_WORDS, max_sequence_len=MAX_SEQUENCE_LEN)
# print(story)

perplexity_dict = {}

beem_story = beam_search(seed_text, max_words=MAX_WORDS, max_sequence_len=MAX_SEQUENCE_LEN, model=model, tokenizer=tokenizer, beam_width=3)
print("generate by beem search:\n" + beem_story)
sequences = [seed_text, beem_story]
# print(sequences)
perplexity = calculate_perplexity(model = model, tokenizer=tokenizer, sequences=sequences, max_sequence_len=MAX_SEQUENCE_LEN)
print("beem search Perplexity: " + str(perplexity))
bleu_score = calculate_bleu_score(sequences)
perplexity_dict["beam_search"] = perplexity
print("beem search BLEU Score: " + str(bleu_score))

greed_stroy = greedy_search(seed_text, max_words=MAX_WORDS, max_sequence_len=MAX_SEQUENCE_LEN, model=model, tokenizer=tokenizer)
print("generate by greedy search:\n" + greed_stroy)
sequences = [seed_text, greed_stroy]
# print(sequences)
perplexity = calculate_perplexity(model = model, tokenizer=tokenizer, sequences=sequences, max_sequence_len=MAX_SEQUENCE_LEN)
print("greedy search Perplexity: " + str(perplexity))
bleu_score = calculate_bleu_score(sequences)
perplexity_dict["greedy_search"] = perplexity
print("greedy search BLEU Score: " + str(bleu_score))


top_k_story = top_k_sampling(seed_text, max_words=MAX_WORDS, max_sequence_len=MAX_SEQUENCE_LEN, model=model, tokenizer=tokenizer, k=3)
print("generate by top k sampling:\n" + top_k_story)
sequences = [seed_text, top_k_story]
# print(sequences)
perplexity = calculate_perplexity(model = model, tokenizer=tokenizer, sequences=sequences, max_sequence_len=MAX_SEQUENCE_LEN)
print("top_k_story search Perplexity: " + str(perplexity))
bleu_score = calculate_bleu_score(sequences)
perplexity_dict["top_k"] = perplexity
print("top_k_story search BLEU Score: " + str(bleu_score))


top_p_story = top_p_sampling(seed_text, max_words=MAX_WORDS, max_sequence_len=MAX_SEQUENCE_LEN, model=model, tokenizer=tokenizer, p=0.8)
print("generate by top p sampling:\n" + top_p_story)
sequences = [seed_text, top_p_story]
# print(sequences)
perplexity = calculate_perplexity(model = model, tokenizer=tokenizer, sequences=sequences, max_sequence_len=MAX_SEQUENCE_LEN)
print("top_p_story search Perplexity: " + str(perplexity))
bleu_score = calculate_bleu_score(sequences)
perplexity_dict["top_p"] = perplexity
print("top_p_story search BLEU Score: " + str(bleu_score))
print(perplexity_dict)
